{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5df9173-030c-4638-a715-48436eefca40",
   "metadata": {},
   "source": [
    "# An Introductory Neural Network with JAX\n",
    "We will now build a relatively basic neural network to classify $28\\times28$ images of written characters between zero and nine from the MNIST training set. It's largely derived from the [Training a Simple Neural Network, with PyTorch Data Loading tutorial](https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html).  The Pytorch data loading has been cut out, though.\n",
    "\n",
    "Let's load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "1cff3c38-6f76-46dd-b061-2d1d578c070f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "import gzip\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# Download from here if you must: https://github.com/pytorch/tutorials/blob/main/_static/mnist.pkl.gz\n",
    "mnist_path = Path('data') / 'mnist' / 'mnist.pkl.gz'\n",
    "\n",
    "with gzip.open(mnist_path, 'rb') as f:\n",
    "    data = pickle.load(f, encoding='latin-1')\n",
    "\n",
    "# First, split the data set into training and validation subsets.\n",
    "train_d = tuple(map(jnp.array, data[0]))\n",
    "valid_d = tuple(map(jnp.array, data[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ddc61f-b6b3-4fc2-b3b6-018fb675528c",
   "metadata": {},
   "source": [
    "Each subset is a tuple of length 2.  The first component is a 2D array where each row is a flattened image.  The second component is a 1D arry of integers, each corresponding to a row in the first component.  These integers are the labels. \n",
    "\n",
    "We are going to optimze a function $y_i=f(\\mathbf{x}_i|\\mathbf{\\theta})$, where $\\mathbf{x}_i$ is the $i^{th}$ row of the input data array and $y_i$ is its corresponding label. $\\mathbf{\\theta}$ is a vector of parameters to optimize, which will then provide a good approximation of unknown true classifier function. \n",
    "\n",
    "Before we do this, let's transform the label vector into a label array of zeros and ones via one-hot encoding. The idea is pretty straightfoward.  Each row of the new label array will be a length-$10$ vector (recall, all data has ten possible labels) with all components set to zeros except for the index correspondinging to the initial label vector's integer value.  Consider a binary classification example (so only two possible labels) on a data set with three elements.  The one-hot encoding would look like this:\n",
    "\n",
    "$\\begin{bmatrix} 1 \\\\ 2 \\\\ 2\\end{bmatrix}\\rightarrow \\begin{bmatrix}1 & 0\\\\0 & 1\\\\ 0 & 1 \\end{bmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "214a16fc-2309-411d-9073-3c095deae46a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray([[0],\n",
       "              [2],\n",
       "              [3],\n",
       "              [2],\n",
       "              [1],\n",
       "              [2]], dtype=int32),\n",
       " DeviceArray([[1., 0., 0., 0.],\n",
       "              [0., 0., 1., 0.],\n",
       "              [0., 0., 0., 1.],\n",
       "              [0., 0., 1., 0.],\n",
       "              [0., 1., 0., 0.],\n",
       "              [0., 0., 1., 0.]], dtype=float32))"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels = jnp.array([0, 2, 3, 2, 1, 2])\n",
    "\n",
    "# Luckily, JAX has a built in one-hot transformation function.\n",
    "onehot_test_labels = jax.nn.one_hot(test_labels, 4)\n",
    "\n",
    "# Let's see what the test looks like.\n",
    "jnp.expand_dims(test_labels, (1,)), onehot_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a26a0c8f-fa59-4df1-8991-61ddcd85549d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize data into training/validation input and output pairs.\n",
    "train_in = train_d[0]\n",
    "train_out = jax.nn.one_hot(train_d[1], jnp.max(train_d[1]) + 1)\n",
    "valid_in = valid_d[0]\n",
    "valid_out = jax.nn.one_hot(valid_d[1], jnp.max(valid_d[1]) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf61c36-3a5a-4ccb-8cbe-db2510339822",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Autobatching the Data\n",
    "Batches of `bs=100` images will be fed through the network at a time during training.  Let's write a function that will autobatch the image-label pairs, because right now the format is for one large batch with `bs=len(train_in)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ece7ecb0-9384-4d35-a958-6ae9e7e8c915",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 784) (10, 10)\n",
      "(10, 784) (10, 10)\n",
      "(10, 784) (10, 10)\n"
     ]
    }
   ],
   "source": [
    "def autobatch(x, y, bs):\n",
    "    n_data = len(x)\n",
    "    n_b = n_data // bs\n",
    "    x_b = [x[i * bs:(i + 1) * bs] for i in range(n_b - 1)]\n",
    "    y_b = [y[i * bs:(i + 1) * bs] for i in range(n_b - 1)]\n",
    "\n",
    "    if n_data % bs:\n",
    "        x_b.append(x[n_b * bs:])\n",
    "        y_b.append(y[n_b * bs:])\n",
    "    return zip(x_b, y_b)\n",
    "\n",
    "# Test to make sure it works. \n",
    "for x, y in autobatch(train_in[:40], train_out[:40], 10):\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9b04ed-9dfb-4dfa-875e-aa188eb138be",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Neural Network Machinery and Random Keys\n",
    "The mapping between layers is linear, which means for an input $\\mathbf{x}\\in \\mathcal{R}^n$, the layer output $\\mathbf{y}\\in\\mathcal{R}^m$ is a linear transformation $\\mathbf{y}=\\mathbf{W}\\mathbf{x}+\\mathbf{b}$.  The $n\\times m$ matrix $\\mathbf{W}$ contain the layer's weight parameters, and the length-$m$ vector $\\mathbf{b}$ contain its bias parameters. This model will have three such layers: an input layer, a hidden layer, and an output layer. \n",
    "\n",
    "We are going to create some helper functions to instantiate layers of this neural network.  To do so requires some JAX-style random number generation.  \n",
    "\n",
    "Recall that JAX handles (pseudo)random number generators a little differentt than numpy or scipy.  You need to provide a random seed (or key in JAX nomenclature) for reach random object that is instantiated.  If you keep reusing the same key, though, you'll just get the same 'random' numbers over and over, though.  This is where `jax.random.split` comes to the rescue.  \n",
    "\n",
    "In a JAX workflow, a random key is first fed into `jax.random.split`, which returns a tuple of new keys `(key, subkey)`.  The recommended workflow is to feed `subkey` into a random number generator and forget about it, and use `key` to the generate a new pair from another `jax.random.split` call.  There is more information about this via a [tutorial](https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html).\n",
    "\n",
    "First thing to do is to define a function that returns weights and biases for a network layer.  We'll have to instantiate some random JAX arrays to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c94ba4df-2119-45cc-a9c4-2309dd408254",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This returns weights and biases for a dense neural network layer.\n",
    "def random_layer_params(key, m, n, scale=1e-2):\n",
    "    # Let's cheat a bit --- we don't need to keep the new key, so let's feed it into\n",
    "    # one of the two returned JAX arrays.\n",
    "    weight_key, bias_key = jax.random.split(key)\n",
    "    return scale * jax.random.normal(weight_key, (n, m)), scale * jax.random.normal(bias_key, (n,))\n",
    "\n",
    "\n",
    "# This returns an instantiated list of weights and biases.\n",
    "def init_network_params(key, sizes):\n",
    "    # We need a key for each layer. We get multiple (key,subkey) pairs providing an integer.\n",
    "    keys = jax.random.split(key, len(sizes))\n",
    "    return [random_layer_params(k, m, n) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38c021b-80d2-4f26-bbd8-7ae85dca1d84",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "A neural network needs activation functions between its layers.  Here, an ReLU nonlinear activation function will be used.  ReLU is a  straightforward piecewise function:\n",
    "\n",
    "$\\text{ReLU}(x)=\\begin{cases} x & x\\geq 0 \\\\ 0 & x<0 \\end{cases}$.\n",
    "\n",
    "It can also be thought of as a maximum comparison function where an input is compared to zero.  \n",
    "\n",
    "We also want to define a function that returns a prediction for a single input image. Because there are multiple layers, we activate each layer starting with the input image as the activation, transform it via the first layer, and redefine the activation as this layer's output, and then feed in the new activation into the next layer. This process is continued until all layers are used an a final output is provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b7c95edc-c637-4314-9657-a4cb89d9b72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,) (10,)\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [28 * 28, 512, 512, 10] # network layer sizes\n",
    "test_params = init_network_params(jax.random.PRNGKey(99), layer_sizes)\n",
    "\n",
    "def ReLU(x):\n",
    "    return jnp.maximum(0., x)\n",
    "\n",
    "def prediction(params, image):\n",
    "    activation = image\n",
    "    for w, b in params[:-1]:\n",
    "        output = jnp.dot(w, activation) + b\n",
    "        activation = ReLU(output) \n",
    "    \n",
    "    # We don't want to run the output through the activation function.\n",
    "    w_final, b_final = params[-1]\n",
    "    logits = jnp.dot(w_final, activation) + b_final\n",
    "    # print(logits, jax.scipy.special.logsumexp(logits))\n",
    "    return logits - jax.scipy.special.logsumexp(logits)\n",
    "\n",
    "# Let's test on a random 28x28 image.\n",
    "test_image = jax.random.normal(jax.random.PRNGKey(1), (28 * 28,))\n",
    "test_pred = prediction(test_params, test_image)\n",
    "print(test_image.shape, test_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1f4e22-fc74-4a74-93a1-52f50e711ead",
   "metadata": {},
   "source": [
    "Let's step back and decipher what is going on in `prediction`.  In the first step, we define a new variable `activation` and feed it through the respective layers of the neural network, updating the activation input at the end of each iteration.  The last layer defines the output, which is called `logits`, which can be interpreted as an unnormalized score.  \n",
    "\n",
    "We do not return the `logits` array, though.  It is normalized by subtracting `logsumexp(logits)` to improve numerical stability!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3557e39d-a500-4ff1-b4e1-53e68192c300",
   "metadata": {},
   "source": [
    "## Vectorizing `Prediction`\n",
    "JAX provides a useful transformations `vmap` to vectorize functions.  This will be convenient, since batches of training data will be fed into the `prediction` function.  [A more elementary JAX tutorial](https://jax.readthedocs.io/en/latest/jax-101/03-vectorization.html) introduces how and why to use `vmap`, but in summary, vectorized operations offer superior performance to loops in python.  By vectorizing the `for`-loop in `prediction` with `vmap`, we get a performance boost without having to smash our head against the wall figuring out how to vectorize it directly! \n",
    "\n",
    "We also want the vectorized `prediction` function to handle batched inputs.  Per epoch, each training iteration will take a batch of 100 images.  Vectorizing `prediction` to accomodate this will make life easier.  Let's explore some examples before applying it to `prediction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "88876855-30e8-49be-b8a5-742591750567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4. 6.]\n"
     ]
    }
   ],
   "source": [
    "# Let's vectorize a sum function.\n",
    "def s(x1, x2):\n",
    "    return x1 + x2\n",
    "\n",
    "# Let's sum two vectors element-wise.\n",
    "x = jnp.array([1., 2.])\n",
    "y = jnp.array([3., 4.])\n",
    "s_vmap = jax.vmap(s, in_axes=(0, 0))\n",
    "print(s_vmap(x, y)) # looks good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8c7b37a2-d00f-4cf0-858b-ddaa8eeea898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.  8.]\n",
      " [10. 12.]]\n"
     ]
    }
   ],
   "source": [
    "# What happens when we try to sum two matrices with s_vmap.\n",
    "a = jnp.array([[1., 2.], [3., 4.]])\n",
    "b = jnp.array([[5., 6.], [7., 8.]])\n",
    "print(s_vmap(a, b)) # looks good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "38ff91c0-22de-41f6-9f69-4a4b86ffeb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4. 5.]\n",
      " [5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "# We can change is to the first argument is ignored.\n",
    "s_vmap_2 = jax.vmap(s, in_axes=(None, 0))\n",
    "# Now, the first argument is being held fixed as an input array while the second is iterated over.\n",
    "# That means we get an output of two vectors: [1, 2] + 3 and [1, 2] + 4.\n",
    "print(s_vmap_2(x, y)) # looks good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b7977f0a-8d95-4ac6-84a7-c6e69870deaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 4.]\n",
      " [4. 6.]]\n"
     ]
    }
   ],
   "source": [
    "# That means s_vmap_2(x, a) should yield a 2x2 matrix: [1, 2] + [1, 2] and [1, 2] + [3, 4].\n",
    "print(s_vmap_2(x, a)) # looks good. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa7d87d-bf12-4d58-a2ec-8363ec2da8f0",
   "metadata": {},
   "source": [
    "The vectorized `prediction` function should leave the first argument fixed because only images need to be interated over with a vectorized function.  That means `in_axes=(None, 0)` will provide the correct functionality.\n",
    "\n",
    "Some utility functions will also come in handy.  In particular, we want an accuracy calculator, a loss function, and a function to update parameters via gradient descent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "500ed700-d93f-4707-b2ee-1ac7dcfd271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = jax.random.normal(jax.random.PRNGKey(42), (10, 28 * 28))\n",
    "test_labels = jax.nn.one_hot(\n",
    "    jax.random.randint(jax.random.PRNGKey(42), (10,), minval=0, maxval=9),\n",
    "    10\n",
    ")\n",
    "batched_prediction = jax.vmap(prediction, in_axes=(None, 0))\n",
    "\n",
    "def accuracy(params, images, true_labels):\n",
    "    pred_class = jnp.argmax(batched_prediction(params, images), axis=1)\n",
    "    true_class = jnp.argmax(true_labels, axis=1)\n",
    "    return jnp.mean(pred_class == true_class)\n",
    "\n",
    "def loss(params, images, true_labels):\n",
    "    pred = batched_prediction(params, images)\n",
    "    # print(pred, true_labels)\n",
    "    return -jnp.mean(pred * true_labels) # cross entropy\n",
    "\n",
    "# Decorate the update function so that it is JIT-ified.\n",
    "@jax.jit\n",
    "def update(params, x, y):\n",
    "    gradients = jax.grad(loss)(params, x, y) # This is a tuple\n",
    "    return [(w - dw * lr, b - db * lr) for (w, b), (dw, db) in zip(params, gradients)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61c1732-499e-41e4-8409-810f0ec43b9b",
   "metadata": {},
   "source": [
    "### Digesting `update`\n",
    "That last function has a lot going on in the return statement.  What we are doing is calculating the gradient of the loss function with respect to the model parameters.  Remember that the argument `params` is a tuple `(weights, bias)`, so the gradient of the loss w.r.t. `params` is also a tuple.  The return value is a list where the parameters are adjusted by their respective gradients multiplied by the learning rate `lr`. \n",
    "\n",
    "## Training the Model\n",
    "Let's train.  First, plot an image from and its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "7165cec0-bc48-4706-a0cb-dd76811b0969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "test_index = 123\n",
    "plt.imshow(train_in[test_index].reshape((28, 28))) # Remember, we flattend the images before!\n",
    "label = jnp.argmax(train_out[test_index])\n",
    "plt.title(f'\"{label}\"')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbdaf61-7e9c-4ef2-8a04-bac37f426c2b",
   "metadata": {},
   "source": [
    "## The Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "631da6c0-93f4-4219-901c-e586d2f85f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 time: 2.202 seconds.\n",
      "Training sample accuracy: 0.859.  Training loss: 0.045.\n",
      "Validation sample accuracy: 0.875.  Validation loss: 0.042.\n",
      "Epoch 1 time: 2.020 seconds.\n",
      "Training sample accuracy: 0.912.  Training loss: 0.030.\n",
      "Validation sample accuracy: 0.920.  Validation loss: 0.027.\n",
      "Epoch 2 time: 2.006 seconds.\n",
      "Training sample accuracy: 0.939.  Training loss: 0.021.\n",
      "Validation sample accuracy: 0.945.  Validation loss: 0.019.\n",
      "Epoch 3 time: 2.028 seconds.\n",
      "Training sample accuracy: 0.955.  Training loss: 0.015.\n",
      "Validation sample accuracy: 0.957.  Validation loss: 0.015.\n",
      "Epoch 4 time: 2.011 seconds.\n",
      "Training sample accuracy: 0.964.  Training loss: 0.012.\n",
      "Validation sample accuracy: 0.964.  Validation loss: 0.013.\n"
     ]
    }
   ],
   "source": [
    "init_key = jax.random.PRNGKey(11) # get a key to start with.\n",
    "\n",
    "# These are the constants we need for the model.\n",
    "layer_sizes = [28 * 28, 512, 512, 10] # network layer sizes\n",
    "lr = 10. # Learning rate for gradient descent\n",
    "n_epochs = 5 # Number of times to feed data through the network during training\n",
    "bs = 100 # Size of batches to send through network per forward pass\n",
    "n_targets = 10 # Number of labels\n",
    "\n",
    "# Instantiate parameters. \n",
    "params = init_network_params(init_key, layer_sizes)\n",
    "\n",
    "# Loop to train parameters.  Print out some useful information along the way. \n",
    "for epoch in range(n_epochs):\n",
    "    start_time = time.time()\n",
    "    for images, labels in autobatch(train_in, train_out, bs):\n",
    "        params = update(params, images, labels)\n",
    "\n",
    "    # Info to print.\n",
    "    epoch_time = time.time() - start_time\n",
    "    train_accuracy = accuracy(params, train_in, train_out)\n",
    "    train_loss = loss(params, train_in, train_out)\n",
    "    validation_accuracy = accuracy(params, valid_in, valid_out)\n",
    "    validation_loss = loss(params, valid_in, valid_out)\n",
    "    print(f'Epoch {epoch} time: {epoch_time:.3f} seconds.')\n",
    "    print(f'Training sample accuracy: {train_accuracy:.3f}.  Training loss: {train_loss:.3f}.')\n",
    "    print(f'Validation sample accuracy: {validation_accuracy:.3f}.  Validation loss: {validation_loss:.3f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "47b81a58-1197-4d76-987e-274133d6c47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label: 8\n"
     ]
    }
   ],
   "source": [
    "# Let's dispay a test image and print out the predicted label.\n",
    "plt.figure(figsize=(4,4))\n",
    "test_index = 17\n",
    "plt.imshow(train_in[test_index].reshape((28, 28))) # Remember, we flattend the images before!\n",
    "label = jnp.argmax(train_out[test_index])\n",
    "plt.title(f'\"{label}\"')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()\n",
    "print('Prediction label:', jnp.argmax(prediction(params, train_in[test_index])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78855e75-1a17-411a-889c-61582696bce2",
   "metadata": {},
   "source": [
    "## Getting Label Probabilities\n",
    "The model is trained, but its output predictions are not probabilities for each label. All that needs to be done is a renormalization that makes sure the predicted label values sum to one and are all positive.  Luckily, the softmax function does just this.\n",
    "\n",
    "$\\text{SoftMax}(x_i)=\\frac{e^{x_i}}{\\sum_{j=1}^n e^{x_j}}$\n",
    "\n",
    "Here, $x_i\\in\\mathbf{x}$ is a length-$n$ vector.  This mapping preserves order, but normalizes the sum to unity.  It's exactly what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "d313712b-9c0e-44ae-8d3c-4219f4d2870a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-95.2064\n",
      "\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([5.2680139e-07, 5.1918620e-04, 8.2599627e-06, 6.8638896e-05,\n",
       "             7.4309787e-06, 5.0259236e-04, 8.4428245e-07, 8.8851048e-06,\n",
       "             9.9784762e-01, 1.0360671e-03], dtype=float32)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output = prediction(params, train_in[test_index])\n",
    "\n",
    "print(jnp.sum(test_output)) # this does not sum to 1.\n",
    "print()\n",
    "\n",
    "def softmax(x):\n",
    "    return jnp.exp(x) / jnp.sum(jnp.exp(x))\n",
    "\n",
    "test_probs = softmax(test_output)\n",
    "print(jnp.sum(test_probs)) # we now have label probabilities\n",
    "\n",
    "test_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41d409d-f86b-4e3d-8cf2-e8d73b18b801",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
